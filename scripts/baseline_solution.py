
from pathlib import Path
import subprocess
import sys
import pickle
import warnings
from typing import Dict, List, Tuple, Optional

import numpy as np # type: ignore
import pandas as pd # type: ignore
from sklearn.metrics import mean_absolute_error, brier_score_loss, accuracy_score, mean_squared_error
from sklearn.calibration import CalibratedClassifierCV
from sklearn.model_selection import TimeSeriesSplit, cross_val_score
from sklearn.multioutput import MultiOutputRegressor
import catboost as cb
from sklearn.ensemble import GradientBoostingClassifier, RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.feature_extraction.text import TfidfVectorizer
import hashlib
import math
import re
from collections import Counter, defaultdict
from dataclasses import dataclass
from datetime import datetime, timezone
from typing import Iterable, List, Tuple, Dict, Optional


# –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–≥—É–ª—è—Ä–∫–∏ –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏–∏ –≤–∞–∂–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏–µ–π –ø—Ä–∏–±—ã–ª–∏ –æ—Ç –Ω–æ–≤–æ—Å—Ç–µ–π 
RE_SPACES = re.compile(r'\s+')
RE_NUM = re.compile(r'(\d+[.,]?\d*)')
RE_PCT = re.compile(r'(\d+[.,]?\d*)\s?%')
RE_MONEY = re.compile(r'(‚ÇΩ|\$|‚Ç¨|—Ä—É–±|usd|eur|–¥–æ–ª–ª–∞—Ä|–µ–≤—Ä–æ|–º–ª—Ä–¥|–º–ª–Ω|—Ç—ã—Å)\b', re.IGNORECASE)

UP_LEX = [
    r'—Ä–æ—Å—Ç', r'–≤—ã—Ä–æ—Å', r'–ø–æ–≤—ã—à', r'–≤—ã—à–µ –ø—Ä–æ–≥–Ω–æ–∑', r'–æ–¥–æ–±—Ä', r'–ø–æ–∑–∏—Ç–∏–≤', r'—Ä–µ–∫–æ—Ä–¥', r'—Å–æ–≥–ª–∞—Å',
    r'—É—Ç–≤–µ—Ä–¥–∏–ª', r'–ø–æ–ª—É—á–∏–ª –∫–æ–Ω—Ç—Ä–∞–∫—Ç', r'—É–ª—É—á—à', r'–ø–æ–¥–¥–µ—Ä–∂', r'–ø–æ–≥–ª'
]
DOWN_LEX = [
    r'—Å–Ω–∏–∂', r'–Ω–∏–∂–µ –ø—Ä–æ–≥–Ω–æ–∑', r'–ø–∞–¥–µ–Ω', r'—à—Ç—Ä–∞—Ñ', r'—Å–∞–Ω–∫—Ü', r'–∑–∞–ø—Ä–µ—Ç', r'—Ä–∞—Å—Å–ª–µ–¥', r'—É–±—ã—Ç–æ–∫',
    r'—Å—Ä—ã–≤', r'–æ—Å—Ç–∞–Ω–æ–≤', r'–¥–µ–ª–∏—Å—Ç–∏–Ω–≥', r'—Ä–∏—Ç–µ–π–ª\s+—Å–ª–∞–±', r'–ø–ª–æ—Ö'
]

INTENSIFIERS = [r'–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω', r'—Ä–µ–∑–∫–æ', r'–∫—Ä—É–ø–Ω', r'—Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω', r'—Å–∏–ª—å–Ω–æ']

RUMOR_LEX = [
    r'–º–æ–∂–µ—Ç', r'–≤–æ–∑–º–æ–∂–Ω–æ', r'–ø–ª–∞–Ω–∏—Ä', r'–æ–±—Å—É–∂–¥–∞', r'—Ä–∞—Å—Å–º–∞—Ç—Ä–∏', r'–∏—Å—Ç–æ—á–Ω–∏–∫', r'—Å–æ–æ–±—â–∞–µ—Ç—Å—è',
    r'–ø–æ –¥–∞–Ω–Ω—ã–º', r'rumor', r'–æ–∂–∏–¥–∞–µ—Ç—Å—è', r'–ø—Ä–∏–Ω' 
]

EVENT_LEX = {
    'EARN': [r'–≤—ã—Ä—É—á–∫', r'ebitda', r'–ø—Ä–∏–±—ã–ª', r'–º—Å—Ñ–æ', r'—Ä—Å–±—É', r'—Ñ–∏–Ω—Ä–µ–∑—É–ª—å—Ç', r'–º–∞—Ä–∂–∏–Ω', r'–æ–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω', r'—Ä–µ–∑—É–ª—å—Ç–∞—Ç'],
    'GUIDE': [r'–ø—Ä–æ–≥–Ω–æ–∑', r'guidance', r'–æ–∂–∏–¥–∞', r'–ø–µ—Ä–µ—Å–º–æ—Ç—Ä', r'–ø–æ–≤—ã—à–∞–µ—Ç –ø—Ä–æ–≥–Ω–æ–∑', r'—Å–Ω–∏–∂–∞–µ—Ç –ø—Ä–æ–≥–Ω–æ–∑'],
    'MA': [r'–ø–æ–∫—É–ø', r'–ø—Ä–∏–æ–±—Ä–µ—Ç', r'–∫—É–ø–∏—Ç', r'–ø—Ä–æ–¥–∞—Å—Ç', r'—Å–ª–∏—è–Ω', r'–æ–±—ä–µ–¥–∏–Ω', r'–¥–æ–ª—é', r'stake', r'ipo', r'spo', r'–±–∞–π–±–µ–∫', r'–æ–±—Ä–∞—Ç–Ω\w*\s+–≤—ã–∫—É–ø'],
    'DIV': [r'–¥–∏–≤–∏–¥–µ–Ω'],
    'REG': [r'—Ñ–∞—Å', r'—Ä–µ–≥—É–ª—è—Ç–æ—Ä', r'–º–∏–Ω—Ñ–∏–Ω', r'—Ü–±\b|–±–∞–Ω–∫\s+—Ä–æ—Å—Å–∏–∏', r'—Å–∞–Ω–∫—Ü', r'—à—Ç—Ä–∞—Ñ', r'–∑–∞–ø—Ä–µ—Ç', r'–ª–∏—Ü–µ–Ω–∑', r'—Å—É–¥', r'–∏—Å–∫', r'–∞—Ä–±–∏—Ç—Ä–∞–∂', r'—Ä–∞—Å—Å–ª–µ–¥'],
    'PROD': [r'–∫–æ–Ω—Ç—Ä–∞–∫—Ç', r'–ø–æ—Å—Ç–∞–≤–∫', r'–∑–∞–ø—É—Å—Ç', r'–ø—Ä–µ–∑–µ–Ω—Ç–æ–≤–∞–ª', r'—Ç–µ–Ω–¥–µ—Ä', r'–ø—Ä–æ–µ–∫—Ç'],
    'MGMT': [r'–Ω–∞–∑–Ω–∞—á', r'—É–≤–æ–ª', r'—Å–æ–≤–µ—Ç\s+–¥–∏—Ä–µ–∫—Ç–æ—Ä'],
    'MACRO': [r'–Ω–µ—Ñ—Ç—å', r'brent', r'—Ä—É–±–ª', r'–∏–Ω—Ñ–ª—è—Ü', r'—Å—Ç–∞–≤–∫', r'—Ñ—Ä—Å', r'–µ—Ü–±', r'–≤–≤–ø', r'—ç–∫—Å–ø–æ—Ä—Ç', r'–∏–º–ø–æ—Ä—Ç']
}

TIER1_SOURCES = [
    'reuters', 'bloomberg', 'financial times', 'wsj', 'wall street journal',
    '–∏–Ω—Ç–µ—Ä—Ñ–∞–∫—Å', '—Ç–∞cc', 'tacc', '—Ç–∞—Å—Å', '—Ä–±–∫', '–∫–æ–º–º–µ—Ä—Å–∞–Ω—Ç', '–≤–µ–¥–æ–º–æ—Å—Ç–∏'
]


def norm_text(x: str) -> str:
    if not isinstance(x, str):
        return ''
    x = x.strip().lower()
    x = RE_SPACES.sub(' ', x)
    return x


def contains_any(patterns: List[str], text: str) -> bool:
    return any(re.search(pat, text, re.IGNORECASE) for pat in patterns)


def count_hits(patterns: List[str], text: str) -> int:
    return sum(1 for pat in patterns if re.search(pat, text, re.IGNORECASE))


def topic_key(text: str) -> str:
    t = norm_text(text)
    t = re.sub(r'\d+', ' ', t)
    t = re.sub(r'\b(–∏|–≤|–Ω–∞|—Å|–ø–æ|–∫|–æ—Ç|–¥–æ|–∑–∞|–¥–ª—è|–∫–∞–∫|–æ|–æ–±|–±–µ–∑|–ø—Ä–∏|–∏–∑|—É|—á—Ç–æ|—ç—Ç–æ|–∞|–Ω–æ|–∏–ª–∏|–∂–µ|–Ω–µ)\b', ' ', t)
    t = RE_SPACES.sub(' ', t).strip()
    if not t:
        return 'empty'
    return hashlib.sha1(t.encode('utf-8')).hexdigest()[:12]


@dataclass
class NewsLabel:
    direction: str
    magnitude_bucket: int
    horizon: str
    event_type: str
    factuality: float
    source_tier1: int
    rumor: int
    topic_key: str


def detect_event_types(text: str) -> Dict[str, int]:
    hits = {}
    for et, pats in EVENT_LEX.items():
        c = count_hits(pats, text)
        if c > 0:
            hits[et] = c
    if not hits:
        hits['OTHER'] = 1
    return hits


def detect_direction(text: str) -> str:
    up = count_hits(UP_LEX, text)
    dn = count_hits(DOWN_LEX, text)
    if up > dn and up > 0:
        return 'up'
    if dn > up and dn > 0:
        return 'down'
    return 'neutral'


def detect_magnitude_bucket(text: str, event_types: Dict[str, int]) -> int:
    has_pct = bool(RE_PCT.search(text))
    has_money = bool(RE_MONEY.search(text))
    strong_event = any(k in event_types for k in ['REG', 'MA', 'DIV', 'EARN'])
    has_intens = contains_any(INTENSIFIERS, text)

    if (has_pct or has_money) and has_intens:
        return 3
    if (has_pct or has_money) or strong_event:
        return 2
    if contains_any(RUMOR_LEX, text):
        return 1
    return 0


def detect_horizon(event_types: Dict[str, int]) -> str:
    if any(k in event_types for k in ['EARN', 'DIV']):
        return 'd1'
    if any(k in event_types for k in ['GUIDE', 'PROD', 'MGMT']):
        return 'w1'
    if any(k in event_types for k in ['REG', 'MA']):
        return 'm1'
    if 'MACRO' in event_types:
        return 'w1'
    return 'd1'


def detect_factuality(text: str) -> float:
    score = 0.0
    if RE_PCT.search(text) or RE_MONEY.search(text):
        score += 0.3
    if contains_any([r'–æ–ø—É–±–ª–∏–∫–æ–≤–∞–ª', r'—Å–æ–æ–±—â–∏–ª', r'—É—Ç–≤–µ—Ä–¥', r'–ø—Ä–∏–Ω—è–ª', r'–æ–¥–æ–±—Ä–∏–ª', r'–æ—Ç—á–∏—Ç–∞–ª—Å—è'], text):
        score += 0.3
    if contains_any(RUMOR_LEX, text):
        score -= 0.3
    return float(np.clip(score, 0.0, 1.0))


def is_tier1(publication: str) -> int:
    pub = norm_text(publication)
    return int(any(s in pub for s in TIER1_SOURCES))


def label_one(title: str, publication: str) -> NewsLabel:
    text = norm_text(f"{title or ''} {publication or ''}")
    et = detect_event_types(text)

    direction = detect_direction(text)
    magnitude_bucket = detect_magnitude_bucket(text, et)
    horizon = detect_horizon(et)
    factuality = detect_factuality(text)
    source_tier1 = is_tier1(publication or '')
    rumor = int(contains_any(RUMOR_LEX, text))
    tkey = topic_key(title or '')

    return NewsLabel(direction, magnitude_bucket, horizon,
                     max(et, key=et.get), factuality, source_tier1, rumor, tkey)


def winsorize(s: pd.Series, p1=0.01, p99=0.99) -> pd.Series:
    if s.empty:
        return s
    lo = s.quantile(p1)
    hi = s.quantile(p99)
    return s.clip(lower=lo, upper=hi)


def robust_scale(s: pd.Series) -> pd.Series:
    if s.empty:
        return s
    med = s.median()
    iqr = (s.quantile(0.75) - s.quantile(0.25))
    if iqr == 0:
        return s - med
    return (s - med) / iqr


def daily_features(news_df: pd.DataFrame, lookback_days=20, taus=(1.0, 5.0), burst_window=30) -> pd.DataFrame:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–Ω–µ–≤–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö —Ñ–∏—á –±–µ–∑ —Ç–∏–∫–µ—Ä–æ–≤"""
    df = news_df.copy()
    if 'publish_date' not in df.columns:
        raise ValueError("news_df must have 'publish_date' column")
    
    # –í—Ä–µ–º—è ‚Üí UTC –¥–µ–Ω—å
    ts = pd.to_datetime(df['publish_date'], utc=True, errors='coerce')
    df['publish_ts'] = ts
    df['publish_day'] = df['publish_ts'].dt.normalize()

    # –î–µ–¥—É–ø –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö —Å—É—Ç–æ–∫ –ø–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–º—É –∑–∞–≥–æ–ª–æ–≤–∫—É
    df['title_norm'] = df['title'].map(lambda x: re.sub(r'\W+', ' ', norm_text(x)).strip())
    df.sort_values(['publish_day', 'title_norm', 'publish_ts'], inplace=True)
    df = df.drop_duplicates(subset=['publish_day', 'title_norm'], keep='first')

    # –ú–µ—Ç–∫–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ –Ω–æ–≤–æ—Å—Ç–∏
    labels: List[NewsLabel] = [
        label_one(title=row['title'], publication=row.get('publication', ''))
        for _, row in df.iterrows()
    ]
    labdf = pd.DataFrame([l.__dict__ for l in labels])
    df = pd.concat([df.reset_index(drop=True), labdf], axis=1)

    # –ö–∞–ª–µ–Ω–¥–∞—Ä—å
    days = pd.date_range(df['publish_day'].min().normalize(),
                         df['publish_day'].max().normalize(), freq='D', tz='UTC')
    out_rows = []

    # –ü—Ä–µ–¥–ø–æ–¥—Å—á—ë—Ç –¥–Ω–µ–≤–Ω—ã—Ö –∫–æ–ª–∏—á–µ—Å—Ç–≤ –¥–ª—è BurstZ
    cnt_by_day = df.groupby('publish_day').size().reindex(days, fill_value=0)
    med_30 = cnt_by_day.rolling(window=burst_window, min_periods=1).median()
    mad_30 = (cnt_by_day - med_30).abs().rolling(window=burst_window, min_periods=1).median()
    mad_30 = mad_30.replace(0, 1.0)

    # –ü—Ä–µ–¥–ø–æ—Å—á–∏—Ç–∞—Ç—å s, g, conf, weights
    s = df['direction'].map({'up': 1, 'down': -1, 'neutral': 0}).fillna(0).astype(float).values
    g_map = {0: 0.0, 1: 0.5, 2: 1.0, 3: 1.5}
    g = df['magnitude_bucket'].map(g_map).fillna(0.0).astype(float).values
    conf = (df['factuality'].clip(0, 1) * (0.5 + 0.5 * df['source_tier1'].astype(int))).values
    conf = conf * np.where(df['rumor'].astype(int).values == 1, 0.5, 1.0)

    # –°—á—ë—Ç—á–∏–∫–∏ –ø–æ —Ç–∏–ø–∞–º/–≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–º –¥–ª—è –æ–∫–Ω–∞
    et_onehot = pd.get_dummies(df['event_type'], prefix='type')
    hz_onehot = pd.get_dummies(df['horizon'], prefix='horz')
    
    for col in ['type_EARN', 'type_GUIDE', 'type_REG', 'type_MA', 'type_DIV']:
        if col not in et_onehot.columns:
            et_onehot[col] = 0
    for col in ['horz_d1', 'horz_w1', 'horz_m1']:
        if col not in hz_onehot.columns:
            hz_onehot[col] = 0
    et_onehot = et_onehot.reset_index(drop=True)
    hz_onehot = hz_onehot.reset_index(drop=True)

    # –î–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞ –∫ –¥–∞–Ω–Ω—ã–º –ø–æ –æ–∫–Ω—É
    df_idx_by_day = defaultdict(list)
    for i, d in enumerate(df['publish_day'].values):
        df_idx_by_day[d].append(i)

    # –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –ø–æ –¥–Ω—è–º
    for t in days:
        # –û–∫–Ω–æ [t-K, t-1]
        left = t - pd.Timedelta(days=lookback_days)
        window_days = pd.date_range(left, t - pd.Timedelta(days=1), freq='D', tz='UTC')
        idxs = []
        for d in window_days:
            idxs.extend(df_idx_by_day.get(d, []))

        if idxs:
            ref = t
            dt_days = (ref - df.iloc[idxs]['publish_day']).dt.days.values.astype(float)
            
            # —è–¥—Ä–∞
            phi_vals = {}
            vol_vals = {}
            for tau in taus:
                k = np.exp(-np.maximum(dt_days, 1.0) / float(tau))
                imp = s[idxs] * g[idxs] * conf[idxs] * k
                phi_vals[f'news_net_tau{tau:g}d'] = float(imp.sum())
                vol_vals[f'news_vol_tau{tau:g}d'] = float(np.abs(imp).sum())

            # –∞–≥—Ä–µ–≥–∞—Ç—ã
            sub_et = et_onehot.iloc[idxs]
            sub_hz = hz_onehot.iloc[idxs]
            type_earn = float(sub_et.get('type_EARN', pd.Series()).sum())
            type_guide = float(sub_et.get('type_GUIDE', pd.Series()).sum())
            type_reg = float(sub_et.get('type_REG', pd.Series()).sum())
            type_ma = float(sub_et.get('type_MA', pd.Series()).sum())
            type_div = float(sub_et.get('type_DIV', pd.Series()).sum())

            total_w = max(1.0, len(idxs))
            
            row = {
                'day': t,
                'news_count_{}d'.format(lookback_days): int(len(idxs)),
                'burst_z_30d': float(((cnt_by_day.loc[t] - med_30.loc[t]) / (mad_30.loc[t] if mad_30.loc[t] != 0 else 1.0)) if t in cnt_by_day.index else 0.0),
                'horz_d1_share_{}d'.format(lookback_days): float(sub_hz.get('horz_d1', pd.Series()).sum() / total_w),
                'type_earn_{}d'.format(lookback_days): type_earn / total_w,
                'type_guide_{}d'.format(lookback_days): type_guide / total_w,
                'type_reg_{}d'.format(lookback_days): type_reg / total_w,
                'type_ma_{}d'.format(lookback_days): type_ma / total_w,
                'type_div_{}d'.format(lookback_days): type_div / total_w,
                'factuality_avg_{}d'.format(lookback_days): float(df.iloc[idxs]['factuality'].mean()),
                'source_tier1_share_{}d'.format(lookback_days): float(df.iloc[idxs]['source_tier1'].astype(int).mean()),
                'rumor_ratio_{}d'.format(lookback_days): float(df.iloc[idxs]['rumor'].astype(int).mean()),
                'chain_consistency_{}d'.format(lookback_days): 0.0,
                'topic_entropy_{}d'.format(lookback_days): 0.0,
                'max_news_ts_used': pd.to_datetime(df.iloc[idxs]['publish_ts']).max(),
            }
            
            row.update(phi_vals)
            row.update(vol_vals)
            
            # Consistency
            mid_tau = 5.0 if (f'news_net_tau5d' in phi_vals and f'news_vol_tau5d' in vol_vals) else float(list(taus)[0])
            num = abs(row.get(f'news_net_tau{mid_tau:g}d', 0.0))
            den = max(1e-9, row.get(f'news_vol_tau{mid_tau:g}d', 0.0))
            row[f'chain_consistency_{lookback_days}d'] = float(np.clip(num / den, 0.0, 1.0))

            # –≠–Ω—Ç—Ä–æ–ø–∏—è —Ç–µ–º
            tk = df.iloc[idxs]['topic_key'].values.tolist()
            if len(tk) > 0:
                c = Counter(tk)
                p = np.array([v / len(tk) for v in c.values()])
                ent = float(-(p * np.log(p + 1e-12)).sum())
            else:
                ent = 0.0
            row[f'topic_entropy_{lookback_days}d'] = ent

        else:
            # –ø—É—Å—Ç–æ–µ –æ–∫–Ω–æ
            row = {
                'day': t,
                'news_count_{}d'.format(lookback_days): 0,
                'burst_z_30d': 0.0,
                'horz_d1_share_{}d'.format(lookback_days): 0.0,
                'type_earn_{}d'.format(lookback_days): 0.0,
                'type_guide_{}d'.format(lookback_days): 0.0,
                'type_reg_{}d'.format(lookback_days): 0.0,
                'type_ma_{}d'.format(lookback_days): 0.0,
                'type_div_{}d'.format(lookback_days): 0.0,
                'factuality_avg_{}d'.format(lookback_days): 0.0,
                'source_tier1_share_{}d'.format(lookback_days): 0.0,
                'rumor_ratio_{}d'.format(lookback_days): 0.0,
                'chain_consistency_{}d'.format(lookback_days): 0.0,
                'topic_entropy_{}d'.format(lookback_days): 0.0,
                'max_news_ts_used': pd.NaT,
            }
            for tau in taus:
                row[f'news_net_tau{tau:g}d'] = 0.0
                row[f'news_vol_tau{tau:g}d'] = 0.0

        out_rows.append(row)

    feats = pd.DataFrame(out_rows)
    # Winsorize + robust-scale
    for col in [c for c in feats.columns if c.startswith('news_net_') or c.startswith('news_vol_') or c.startswith('burst_z')]:
        feats[col] = robust_scale(winsorize(feats[col].astype(float)))

    return feats


# ============================================================================
# –ö–õ–ê–°–°–´ –†–ï–®–ï–ù–ò–ô
# ============================================================================


class METAStockPredictor:
    """
    META-Stock –ø–æ–¥—Ö–æ–¥ –¥–ª—è –º—É–ª—å—Ç–∏-–≥–æ—Ä–∏–∑–æ–Ω—Ç–Ω–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–∏–±—ã–ª–∏ –Ω–∞ –ø—Ä–æ–º–µ–∂—É—Ç–∫–µ 1-20 –¥–Ω–µ–π.
    
    –ö–ª—é—á–µ–≤—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
    - –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ø—Ä–∏–±—ã–ª–∏ (return) –¥–ª—è –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤ 1-20 –¥–Ω–µ–π
    - TimeSeriesSplit –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    - MultiOutputRegressor –¥–ª—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤—Å–µ—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤
    - –ö–æ–º–±–∏–Ω–∞—Ü–∏—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ –∏ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö —Ñ–∏—á
    """
    
    def __init__(self, horizons: List[int] = None, n_splits: int = 5):
        """
        Args:
            horizons: –°–ø–∏—Å–æ–∫ –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 1-20 –¥–Ω–µ–π)
            n_splits: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ø–ª–∏—Ç–æ–≤ –¥–ª—è TimeSeriesSplit
        """
        self.horizons = horizons or list(range(1, 21))
        self.n_splits = n_splits
        self.model = None
        self.scaler = None
        self.feature_names = []
        self.tscv = TimeSeriesSplit(n_splits=n_splits)
        
    def create_multi_horizon_targets(self, df: pd.DataFrame) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞—Ä–≥–µ—Ç–æ–≤ –¥–ª—è –≤—Å–µ—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤"""
        df = df.copy()
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ç–∏–∫–µ—Ä–∞–º –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Ç–∞—Ä–≥–µ—Ç–æ–≤
        for ticker in df['ticker'].unique():
            mask = df['ticker'] == ticker
            ticker_data = df[mask].copy().sort_values('begin')
            
            # –°–æ–∑–¥–∞–µ–º —Ç–∞—Ä–≥–µ—Ç—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞
            for horizon in self.horizons:
                ticker_data[f'return_{horizon}d'] = (
                    ticker_data['close'].shift(-horizon) / ticker_data['close'] - 1
                )
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º
            for horizon in self.horizons:
                df.loc[mask, f'return_{horizon}d'] = ticker_data[f'return_{horizon}d'].values
        
        return df
    
    def prepare_features(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, List[str]]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –º—É–ª—å—Ç–∏-–≥–æ—Ä–∏–∑–æ–Ω—Ç–Ω–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è"""
        df = df.copy()
        
        # –ë–∞–∑–æ–≤—ã–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        for ticker in df['ticker'].unique():
            mask = df['ticker'] == ticker
            ticker_data = df[mask].copy().sort_values('begin')
            
            # –ú–æ–º–µ–Ω—Ç—É–º—ã —Ä–∞–∑–Ω—ã—Ö –ø–µ—Ä–∏–æ–¥–æ–≤
            for period in [1, 3, 5, 10, 20]:
                ticker_data[f'momentum_{period}d'] = ticker_data['close'].pct_change(period)
            
            # –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
            for period in [5, 10, 20]:
                ticker_data[f'volatility_{period}d'] = (
                    ticker_data['close'].pct_change().rolling(period).std()
                )
            
            # –°–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ
            for period in [5, 10, 20, 50]:
                ticker_data[f'ma_{period}'] = ticker_data['close'].rolling(period).mean()
                ticker_data[f'price_to_ma_{period}'] = ticker_data['close'] / ticker_data[f'ma_{period}']
            
            # RSI
            delta = ticker_data['close'].diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
            rs = gain / loss
            ticker_data['rsi_14d'] = 100 - (100 / (1 + rs))
            
            # Bollinger Bands
            bb_period = 20
            bb_std = 2
            bb_middle = ticker_data['close'].rolling(bb_period).mean()
            bb_std_val = ticker_data['close'].rolling(bb_period).std()
            ticker_data['bb_upper'] = bb_middle + (bb_std_val * bb_std)
            ticker_data['bb_lower'] = bb_middle - (bb_std_val * bb_std)
            ticker_data['bb_position'] = (
                (ticker_data['close'] - ticker_data['bb_lower']) / 
                (ticker_data['bb_upper'] - ticker_data['bb_lower'])
            )
            
            # –û–±—ä–µ–º–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
            ticker_data['volume_ma_20'] = ticker_data['volume'].rolling(20).mean()
            ticker_data['volume_ratio'] = ticker_data['volume'] / ticker_data['volume_ma_20']
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º
            feature_cols = [col for col in ticker_data.columns 
                           if col.startswith(('momentum_', 'volatility_', 'ma_', 'price_to_ma_', 
                                            'rsi_', 'bb_', 'volume_'))]
            for col in feature_cols:
                if col in ticker_data.columns:
                    df.loc[mask, col] = ticker_data[col].values
        
        # –ù–æ–≤–æ—Å—Ç–Ω—ã–µ —Ñ–∏—á–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)
        news_features = [col for col in df.columns if col.startswith(('news_', 'type_', 'horz_', 'burst_'))]
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        all_features = [col for col in df.columns 
                       if col.startswith(('momentum_', 'volatility_', 'ma_', 'price_to_ma_', 
                                        'rsi_', 'bb_', 'volume_')) or col in news_features]
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å NaN –≤ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö
        feature_df = df[all_features].fillna(0)
        
        return feature_df, all_features
    
    def train_with_timeseries_split(self, X: np.ndarray, y: np.ndarray) -> MultiOutputRegressor:
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –≤—Ä–µ–º–µ–Ω–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π"""
        print(f"   üîÑ –û–±—É—á–µ–Ω–∏–µ META-Stock –º–æ–¥–µ–ª–∏ —Å TimeSeriesSplit (n_splits={self.n_splits})...")
        
        # –ë–∞–∑–æ–≤—ã–π —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä
        base_model = RandomForestRegressor(
            n_estimators=200,
            max_depth=10,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=42,
            n_jobs=-1
        )
        
        # MultiOutputRegressor –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤—Å–µ—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
        model = MultiOutputRegressor(base_model)
        
        # –í—Ä–µ–º–µ–Ω–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è
        cv_scores = []
        for fold, (train_idx, val_idx) in enumerate(self.tscv.split(X)):
            X_train, X_val = X[train_idx], X[val_idx]
            y_train, y_val = y[train_idx], y[val_idx]
            
            # –û–±—É—á–µ–Ω–∏–µ –Ω–∞ fold
            model.fit(X_train, y_train)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∏ –æ—Ü–µ–Ω–∫–∞
            y_pred = model.predict(X_val)
            mse = mean_squared_error(y_val, y_pred)
            rmse = np.sqrt(mse)
            cv_scores.append(rmse)
            
            print(f"      Fold {fold + 1}: RMSE = {rmse:.6f}")
        
        # –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö
        model.fit(X, y)
        
        mean_cv_score = np.mean(cv_scores)
        std_cv_score = np.std(cv_scores)
        print(f"   ‚úì CV RMSE: {mean_cv_score:.6f} ¬± {std_cv_score:.6f}")
        
        return model
    
    def predict_returns(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ø—Ä–∏–±—ã–ª–∏ –¥–ª—è –≤—Å–µ—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤"""
        print("\ META-Stock –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ø—Ä–∏–±—ã–ª–∏...")
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        features_df, feature_names = self.prepare_features(df)
        self.feature_names = feature_names
        
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è
        self.scaler = StandardScaler()
        X_scaled = self.scaler.fit_transform(features_df)
        
        # –°–æ–∑–¥–∞–µ–º —Ç–∞—Ä–≥–µ—Ç—ã
        df_with_targets = self.create_multi_horizon_targets(df)
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å NaN –≤ —Ç–∞—Ä–≥–µ—Ç–∞—Ö
        target_cols = [f'return_{h}d' for h in self.horizons]
        valid_mask = ~df_with_targets[target_cols].isna().any(axis=1)
        
        X_train = X_scaled[valid_mask]
        y_train = df_with_targets.loc[valid_mask, target_cols].values
        
        print(f"   üìä –û–±—É—á–∞–µ–º –Ω–∞ {len(X_train)} –ø—Ä–∏–º–µ—Ä–∞—Ö —Å {len(feature_names)} –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏")
        print(f"   üìä –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–±—ã–ª—å –¥–ª—è –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤: {self.horizons}")
        
        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        self.model = self.train_with_timeseries_split(X_train, y_train)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö
        predictions = self.model.predict(X_scaled)
        
       
        result_df = df.copy()
        for i, horizon in enumerate(self.horizons):
            result_df[f'pred_return_{horizon}d'] = predictions[:, i]
        
        return result_df


class BaselineSolution:
    """
    Baseline —Ä–µ—à–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–∫–æ–ª—å–∑—è—â–∏—Ö —Å—Ä–µ–¥–Ω–∏—Ö –∏ –º–æ–º–µ–Ω—Ç—É–º–∞

    –õ–æ–≥–∏–∫–∞:
    1. –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–∫–µ—Ä–∞ –≤—ã—á–∏—Å–ª—è–µ–º –º–æ–º–µ–Ω—Ç—É–º (–∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ü–µ–Ω—ã –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ N –¥–Ω–µ–π)
    2. –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º, —á—Ç–æ —Ç—Ä–µ–Ω–¥ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—Å—è (momentum continuation)
    3. –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Ä–æ—Å—Ç–∞ = —Å–∏–≥–º–æ–∏–¥–∞ –æ—Ç –º–æ–º–µ–Ω—Ç—É–º–∞
    """

    def __init__(self, window_size: int = 5, use_ml: bool = True):
 

        self.window_size = window_size
        self.use_ml = use_ml
        self.models = {}
        self.scalers = {}
        self.feature_names = []

    def load_data(self, train_candles_path: str,
                  test_candles_path: str,
                  train_news_path: str = None,
                  test_news_path: str = None):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –Ω–æ–≤–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π (_2 —Ñ–∞–π–ª—ã –∫–∞–∫ —Ç–µ—Å—Ç–æ–≤—ã–µ)"""
        print("üìä –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")

        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        self.train_df = pd.read_csv(train_candles_path)
        self.train_df['begin'] = pd.to_datetime(self.train_df['begin'])

        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ (—Ñ–∞–π–ª—ã —Å _2)
        self.test_df = pd.read_csv(test_candles_path)
        self.test_df['begin'] = pd.to_datetime(self.test_df['begin'])

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –º–æ–º–µ–Ω—Ç—É–º–∞ (–Ω—É–∂–Ω–∞ –∏—Å—Ç–æ—Ä–∏—è)
        self.full_df = pd.concat([self.train_df, self.test_df], ignore_index=True)
        self.full_df = self.full_df.sort_values(['ticker', 'begin'])

        print(f"   ‚úì Train: {len(self.train_df)} —Å—Ç—Ä–æ–∫")
        print(f"   ‚úì Test: {len(self.test_df)} —Å—Ç—Ä–æ–∫")

        # --- –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –¥–Ω–µ–≤–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö —Ñ–∏—á (–±–µ–∑ —Ç–∏–∫–µ—Ä–∞) —Å –ª–∞–≥–æ–º t-1 ---
        # –ï—Å–ª–∏ –≥–æ—Ç–æ–≤—ã—Ö CSV –Ω–µ—Ç, –ø–æ–ø—Ä–æ–±—É–µ–º –∏—Ö –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å.
        # –°–∫—Ä–∏–ø—Ç-–≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∏—â–µ–º –≤ scripts/news_features_no_ticker.py –∏–ª–∏ –≤ ~/Downloads/news_features_no_ticker.py
        try:
            project_root = Path(__file__).resolve().parents[1]
            processed_dir = project_root / 'data' / 'processed' / 'participants'
            raw_dir = project_root / 'data' / 'raw' / 'participants'
            processed_dir.mkdir(parents=True, exist_ok=True)

            news_train_path = processed_dir / 'train_news_daily_features.csv'
            news_test_path = processed_dir / 'test_news_daily_features.csv'

            # –ê–≤—Ç–æ–≥–µ–Ω–µ—Ä–∞—Ü–∏—è, –µ—Å–ª–∏ —Ñ–∞–π–ª–æ–≤ –Ω–µ—Ç
            def try_generate(news_in: Path, out_csv: Path):
                if out_csv.exists():
                    return
                if not news_in.exists():
                    print(f"   ‚Ñπ –ù–µ –Ω–∞–π–¥–µ–Ω –∏—Å—Ö–æ–¥–Ω—ã–π news CSV: {news_in} ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞—é –∞–≤—Ç–æ–≥–µ–Ω–µ—Ä–∞—Ü–∏—é")
                    return
                try:
                    print(f"    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è news-—Ñ–∏—á –∏–∑ {news_in.name}...")
                    news_df = pd.read_csv(news_in)
                    feat_df = daily_features(news_df, lookback_days=20, taus=(1.0, 5.0))
                    feat_df.to_csv(out_csv, index=False)
                    print(f"    –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω —Ñ–∞–π–ª: {out_csv}")
                except Exception as ge:
                    print(f"    –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å {out_csv.name}: {ge}")

            try_generate(raw_dir / 'train_news.csv', news_train_path)
            try_generate(raw_dir / 'test_news.csv', news_test_path)

            news_feats_list = []
            if news_train_path.exists():
                nf_tr = pd.read_csv(news_train_path)
                news_feats_list.append(nf_tr)
            if news_test_path.exists():
                nf_te = pd.read_csv(news_test_path)
                news_feats_list.append(nf_te)

            if news_feats_list:
                news_feats = pd.concat(news_feats_list, ignore_index=True)
                # –ü—Ä–∏–≤–æ–¥–∏–º –¥–µ–Ω—å –∫ UTC-normalized –∏ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º
                if 'day' in news_feats.columns:
                    news_feats['news_day'] = pd.to_datetime(news_feats['day'], utc=True, errors='coerce').dt.normalize()
                    news_feats = news_feats.drop(columns=['day'])
                else:
                    # –ü–æ–ø—ã—Ç–∫–∞ –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–æ–ª–æ–Ω–∫–∏ –¥–Ω—è
                    for cand in ['publish_day', 'date', 'Day', 'DATE']:
                        if cand in news_feats.columns:
                            news_feats['news_day'] = pd.to_datetime(news_feats[cand], utc=True, errors='coerce').dt.normalize()
                            break
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º begin –¥–æ –¥–Ω—è –∏ —Å–¥–≤–∏–≥–∞–µ–º –Ω–∞ -1 –¥–µ–Ω—å –¥–ª—è –ª–æ–≥–∞ t-1
                self.full_df['begin_day'] = pd.to_datetime(self.full_df['begin'], utc=True, errors='coerce').dt.normalize()
                self.full_df['news_day'] = self.full_df['begin_day'] - pd.Timedelta(days=1)

                # merge –ø–æ –¥–Ω—é (–ª–µ–≤—ã–π join)
                if 'news_day' in news_feats.columns:
                    merged = self.full_df.merge(news_feats, on='news_day', how='left')
                    # –ó–∞–ø–æ–ª–Ω–∏–º –ø—Ä–æ–ø—É—Å–∫–∏ –ø–æ —á–∏—Å–ª–æ–≤—ã–º news-—Ñ–∏—á–∞–º –Ω—É–ª—è–º–∏
                    news_cols = [c for c in merged.columns if c.startswith('news_') or c.startswith('type_') or c.startswith('horz_') or c in (
                        'factuality_avg_20d', 'source_tier1_share_20d', 'rumor_ratio_20d', 'burst_z_30d', 'topic_entropy_20d', 'chain_consistency_20d'
                    )]
                    for col in news_cols:
                        if col in merged.columns:
                            merged[col] = merged[col].fillna(0)
                    self.full_df = merged
                    print("   ‚úì –ü–æ–¥–º–µ—à–∞–Ω—ã –¥–Ω–µ–≤–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–Ω—ã–µ —Ñ–∏—á–∏ (–ª–∞–≥ t-1)")
                else:
                    print("    –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–æ–ª–æ–Ω–∫—É –¥–Ω—è –≤ news-—Ñ–∏—á–∞—Ö ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞—é merge")
            else:
                print("   ‚Ñπ –§–∞–π–ª—ã –¥–Ω–µ–≤–Ω—ã—Ö news-—Ñ–∏—á –Ω–µ –Ω–∞–π–¥–µ–Ω—ã ‚Äî —Ä–∞–±–æ—Ç–∞—é –∫–∞–∫ —á–∏—Å—Ç—ã–π baseline")
        except Exception as e:
            print(f"    –û—à–∏–±–∫–∞ –ø—Ä–∏ merge news-—Ñ–∏—á: {e}. –ü—Ä–æ–¥–æ–ª–∂–∞—é –±–µ–∑ –Ω–æ–≤–æ—Å—Ç–µ–π")

    def compute_features(self):
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–º–æ–º–µ–Ω—Ç—É–º, –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å)"""
        print("\n –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")

        df = self.full_df.copy()

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ç–∏–∫–µ—Ä–∞–º
        for ticker in df['ticker'].unique():
            mask = df['ticker'] == ticker
            ticker_data = df[mask].copy()

            # 1. –ú–æ–º–µ–Ω—Ç—É–º = –ø—Ä–æ—Ü–µ–Ω—Ç–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ü–µ–Ω—ã –∑–∞ window_size –¥–Ω–µ–π
            ticker_data['momentum'] = (
                ticker_data['close'].pct_change(self.window_size)
            )

            # 2. –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å = std –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–µ–π –∑–∞ window_size –¥–Ω–µ–π
            ticker_data['volatility'] = (
                ticker_data['close'].pct_change().rolling(self.window_size).std()
            )

            # 3. –°—Ä–µ–¥–Ω—è—è —Ü–µ–Ω–∞ –∑–∞ window_size –¥–Ω–µ–π
            ticker_data['ma'] = ticker_data['close'].rolling(self.window_size).mean()

            # 4. –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –æ—Ç MA (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ)
            ticker_data['distance_from_ma'] = (
                (ticker_data['close'] - ticker_data['ma']) / ticker_data['ma']
            )

            # –û–±–Ω–æ–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
            df.loc[mask, 'momentum'] = ticker_data['momentum'].values
            df.loc[mask, 'volatility'] = ticker_data['volatility'].values
            df.loc[mask, 'ma'] = ticker_data['ma'].values
            df.loc[mask, 'distance_from_ma'] = ticker_data['distance_from_ma'].values

        self.full_df = df
        print(" –ë–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤—ã—á–∏—Å–ª–µ–Ω—ã")
        
        if self.use_ml:
            self._compute_extended_features()

    def _compute_extended_features(self):
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è ML"""
        print("   üîß –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö ML-–ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        
        df = self.full_df.copy()
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ç–∏–∫–µ—Ä–∞–º –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö —Ñ–∏—á
        for ticker in df['ticker'].unique():
            mask = df['ticker'] == ticker
            ticker_data = df[mask].copy()
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–æ–º–µ–Ω—Ç—É–º—ã
            ticker_data['momentum_1d'] = ticker_data['close'].pct_change(1)
            ticker_data['momentum_10d'] = ticker_data['close'].pct_change(10)
            ticker_data['momentum_20d'] = ticker_data['close'].pct_change(20)
            ticker_data['momentum_60d'] = ticker_data['close'].pct_change(60)
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
            ticker_data['volatility_1d'] = ticker_data['close'].pct_change().rolling(1).std()
            ticker_data['volatility_10d'] = ticker_data['close'].pct_change().rolling(10).std()
            ticker_data['volatility_20d'] = ticker_data['close'].pct_change().rolling(20).std()
            ticker_data['volatility_60d'] = ticker_data['close'].pct_change().rolling(60).std()
            
            # –°–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö –ø–µ—Ä–∏–æ–¥–æ–≤
            ticker_data['ma_10'] = ticker_data['close'].rolling(10).mean()
            ticker_data['ma_20'] = ticker_data['close'].rolling(20).mean()
            ticker_data['ma_60'] = ticker_data['close'].rolling(60).mean()
            
            # RSI (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π)
            delta = ticker_data['close'].diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
            rs = gain / loss
            ticker_data['rsi_14d'] = 100 - (100 / (1 + rs))
            
            # Bollinger Bands
            bb_period = 20
            bb_std = 2
            bb_middle = ticker_data['close'].rolling(bb_period).mean()
            bb_std_val = ticker_data['close'].rolling(bb_period).std()
            ticker_data['bb_upper'] = bb_middle + (bb_std_val * bb_std)
            ticker_data['bb_lower'] = bb_middle - (bb_std_val * bb_std)
            ticker_data['bb_position'] = (ticker_data['close'] - ticker_data['bb_lower']) / (ticker_data['bb_upper'] - ticker_data['bb_lower'])
            
            # –û–±—ä–µ–º–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            ticker_data['volume_ma_20'] = ticker_data['volume'].rolling(20).mean()
            ticker_data['volume_ratio_20d'] = ticker_data['volume'] / ticker_data['volume_ma_20']
            
            # –ü–æ–∑–∏—Ü–∏—è —Ü–µ–Ω—ã –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ
            ticker_data['high_20d'] = ticker_data['high'].rolling(20).max()
            ticker_data['low_20d'] = ticker_data['low'].rolling(20).min()
            ticker_data['price_position_20d'] = (ticker_data['close'] - ticker_data['low_20d']) / (ticker_data['high_20d'] - ticker_data['low_20d'])
            
            # –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è MA
            ticker_data['ma_cross_5_20'] = (ticker_data['ma'] > ticker_data['ma_20']).astype(int)
            ticker_data['ma_cross_10_20'] = (ticker_data['ma_10'] > ticker_data['ma_20']).astype(int)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
            for col in ['momentum_1d', 'momentum_10d', 'momentum_20d', 'momentum_60d',
                       'volatility_1d', 'volatility_10d', 'volatility_20d', 'volatility_60d',
                       'ma_10', 'ma_20', 'ma_60', 'rsi_14d', 'bb_position',
                       'volume_ratio_20d', 'price_position_20d', 'ma_cross_5_20', 'ma_cross_10_20']:
                if col in ticker_data.columns:
                    df.loc[mask, col] = ticker_data[col].values
        
        # –ö—Ä–æ—Å—Å-—Ñ–∏—á–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å –Ω–æ–≤–æ—Å—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ)
        if 'news_net_tau1d' in df.columns:
            df['momentum_5d_x_news_net_tau1d'] = df['momentum'] * df['news_net_tau1d'].fillna(0)
            df['volatility_20d_x_news_vol_tau5d'] = df['volatility_20d'].fillna(0) * df['news_vol_tau5d'].fillna(0)
            df['volume_ratio_20d_x_burst_z_30d'] = df['volume_ratio_20d'].fillna(1) * df['burst_z_30d'].fillna(0)
        
        self.full_df = df
        print("   ‚úì –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤—ã—á–∏—Å–ª–µ–Ω—ã")

    def _prepare_features(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, List[str]]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è ML"""
        # –ë–∞–∑–æ–≤—ã–µ —Ü–µ–Ω–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        price_features = [
            'momentum', 'momentum_1d', 'momentum_10d', 'momentum_20d', 'momentum_60d',
            'volatility', 'volatility_1d', 'volatility_10d', 'volatility_20d', 'volatility_60d',
            'distance_from_ma', 'rsi_14d', 'bb_position', 'volume_ratio_20d', 'price_position_20d',
            'ma_cross_5_20', 'ma_cross_10_20'
        ]
        
        # –ù–æ–≤–æ—Å—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        news_features = [
            'news_net_tau1d', 'news_net_tau5d', 'news_vol_tau1d', 'news_vol_tau5d',
            'news_count_20d', 'burst_z_30d', 'type_earn_20d', 'type_guide_20d', 'type_reg_20d',
            'type_ma_20d', 'type_div_20d', 'factuality_avg_20d', 'source_tier1_share_20d',
            'rumor_ratio_20d', 'topic_entropy_20d', 'chain_consistency_20d'
        ]
        
        # –ö—Ä–æ—Å—Å-—Ñ–∏—á–∏
        cross_features = [
            'momentum_5d_x_news_net_tau1d', 'volatility_20d_x_news_vol_tau5d', 'volume_ratio_20d_x_burst_z_30d'
        ]
        
        # –°–æ–±–∏—Ä–∞–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        all_features = price_features + news_features + cross_features
        available_features = [f for f in all_features if f in df.columns]
        
        # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏
        feature_df = df[available_features].copy()
        feature_df = feature_df.fillna(0)
        
        return feature_df, available_features

    def _create_targets(self, df: pd.DataFrame) -> Dict[str, pd.Series]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞—Ä–≥–µ—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        targets = {}
        
        # –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–∫–µ—Ä–∞ –≤—ã—á–∏—Å–ª—è–µ–º –±—É–¥—É—â–∏–µ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏
        for ticker in df['ticker'].unique():
            mask = df['ticker'] == ticker
            ticker_data = df[mask].copy().sort_values('begin')
            
            # –î–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏
            ticker_data['return_1d'] = ticker_data['close'].shift(-1) / ticker_data['close'] - 1
            ticker_data['return_20d'] = ticker_data['close'].shift(-20) / ticker_data['close'] - 1
            
            # –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è
            ticker_data['direction_1d'] = (ticker_data['return_1d'] > 0).astype(int)
            ticker_data['direction_20d'] = (ticker_data['return_20d'] > 0).astype(int)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–µ
            df.loc[mask, 'return_1d'] = ticker_data['return_1d'].values
            df.loc[mask, 'return_20d'] = ticker_data['return_20d'].values
            df.loc[mask, 'direction_1d'] = ticker_data['direction_1d'].values
            df.loc[mask, 'direction_20d'] = ticker_data['direction_20d'].values
        
        return {
            'return_1d': df['return_1d'],
            'return_20d': df['return_20d'],
            'direction_1d': df['direction_1d'],
            'direction_20d': df['direction_20d']
        }

    def train_ml_models(self):
        """–û–±—É—á–µ–Ω–∏–µ CatBoost –º–æ–¥–µ–ª–µ–π"""
        if not self.use_ml:
            return
            
        print("\nü§ñ –û–±—É—á–µ–Ω–∏–µ CatBoost –º–æ–¥–µ–ª–µ–π...")
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Ç–æ–ª—å–∫–æ –¥–ª—è train
        train_data = self.full_df[
            self.full_df['begin'].isin(self.train_df['begin'])
        ].copy()
        
        # –°–æ–∑–¥–∞–µ–º —Ç–∞—Ä–≥–µ—Ç—ã
        targets = self._create_targets(train_data)
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        features_df, feature_names = self._prepare_features(train_data)
        self.feature_names = feature_names
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å NaN –≤ —Ç–∞—Ä–≥–µ—Ç–∞—Ö
        valid_mask = ~(targets['return_1d'].isna() | targets['return_20d'].isna())
        features_df = features_df[valid_mask]
        targets = {k: v[valid_mask] for k, v in targets.items()}
        
        print(f"   üìä –û–±—É—á–∞–µ–º –Ω–∞ {len(features_df)} –ø—Ä–∏–º–µ—Ä–∞—Ö —Å {len(feature_names)} –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏")
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã CatBoost
        catboost_params = {
            'iterations': 200,
            'depth': 6,
            'learning_rate': 0.1,
            'l2_leaf_reg': 3,
            'random_seed': 42,
            'verbose': False,
            'thread_count': -1
        }
        
        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¥–ª—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–µ–π
        for target_name in ['return_1d', 'return_20d']:
            print(f"   üîÑ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è {target_name}...")
            
            model = cb.CatBoostRegressor(**catboost_params)
            model.fit(features_df, targets[target_name])
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ train –¥–ª—è –æ—Ü–µ–Ω–∫–∏
            train_pred = model.predict(features_df)
            mae = mean_absolute_error(targets[target_name], train_pred)
            print(f"      MAE –Ω–∞ train: {mae:.6f}")
            
            self.models[target_name] = model
        
        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        for target_name in ['direction_1d', 'direction_20d']:
            print(f"   üîÑ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è {target_name}...")
            
            model = cb.CatBoostClassifier(**catboost_params)
            model.fit(features_df, targets[target_name])
            
            # –ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
            calibrated_model = CalibratedClassifierCV(model, method='isotonic', cv=3)
            calibrated_model.fit(features_df, targets[target_name])
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ train –¥–ª—è –æ—Ü–µ–Ω–∫–∏
            train_pred_proba = calibrated_model.predict_proba(features_df)[:, 1]
            train_pred_binary = calibrated_model.predict(features_df)
            
            brier = brier_score_loss(targets[target_name], train_pred_proba)
            accuracy = accuracy_score(targets[target_name], train_pred_binary)
            
            print(f"      Brier –Ω–∞ train: {brier:.6f}, Accuracy: {accuracy:.4f}")
            
            self.models[target_name] = calibrated_model
        
        print("   ‚úì –ú–æ–¥–µ–ª–∏ –æ–±—É—á–µ–Ω—ã –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")

    def predict(self):
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        
        –ï—Å–ª–∏ use_ml=True: –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±—É—á–µ–Ω–Ω—ã–µ CatBoost –º–æ–¥–µ–ª–∏
        –ò–Ω–∞—á–µ: –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —ç–≤—Ä–∏—Å—Ç–∏–∫—É baseline
        """
        print("\nüéØ –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π...")

        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ test –¥–∞–Ω–Ω—ã–µ
        test_data = self.full_df[
            self.full_df['begin'].isin(self.test_df['begin'])
        ].copy()

        if self.use_ml and self.models:
            # ML –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            print("   ü§ñ –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—É—á–µ–Ω–Ω—ã–µ CatBoost –º–æ–¥–µ–ª–∏...")
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
            features_df, _ = self._prepare_features(test_data)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–µ–π
            test_data['pred_return_1d'] = self.models['return_1d'].predict(features_df)
            test_data['pred_return_20d'] = self.models['return_20d'].predict(features_df)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
            test_data['pred_prob_up_1d'] = self.models['direction_1d'].predict_proba(features_df)[:, 1]
            test_data['pred_prob_up_20d'] = self.models['direction_20d'].predict_proba(features_df)[:, 1]
            
            print("   ‚úì ML –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≥–æ—Ç–æ–≤—ã")
            
        else:
            # Baseline —ç–≤—Ä–∏—Å—Ç–∏–∫–∞
            print("   üìä –ò—Å–ø–æ–ª—å–∑—É–µ–º baseline —ç–≤—Ä–∏—Å—Ç–∏–∫—É...")
            
            # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN –Ω—É–ª—è–º–∏ (–¥–ª—è –ø–µ—Ä–≤—ã—Ö —Å—Ç—Ä–æ–∫ –≥–¥–µ –Ω–µ—Ç –∏—Å—Ç–æ—Ä–∏–∏)
            test_data['momentum'] = test_data['momentum'].fillna(0)
            test_data['volatility'] = test_data['volatility'].fillna(0.01)
            test_data['distance_from_ma'] = test_data['distance_from_ma'].fillna(0)

            # –ù–æ–≤–æ—Å—Ç–Ω–æ–π —Å–∏–≥–Ω–∞–ª (–µ—Å–ª–∏ —Ñ–∏—á–∏ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç –ø–æ—Å–ª–µ merge)
            news_signal = 0.0
            if 'news_net_tau1d' in test_data.columns:
                news_signal = news_signal + test_data['news_net_tau1d'].fillna(0.0)
            if 'news_net_tau5d' in test_data.columns:
                news_signal = news_signal + 0.5 * test_data['news_net_tau5d'].fillna(0.0)

            # –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –≤–ª–∏—è–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π
            try:
                if not isinstance(news_signal, (int, float)):
                    nz_share = float((news_signal != 0).mean())
                    print(f"   üîç News signal coverage (–¥–æ–ª—è –Ω–µ–Ω—É–ª–µ–≤—ã—Ö): {nz_share:.3f}")
                    desc = news_signal.abs().describe(percentiles=[0.5, 0.75, 0.9]).to_dict()
                    print("   üîç |news_signal| stats:", {k: (float(v) if pd.notna(v) else None) for k, v in desc.items()})

                    # –¢–æ–ø-—Ç–∏–∫–µ—Ä—ã –ø–æ –¥–æ–ª–µ –Ω–µ–Ω—É–ª–µ–≤—ã—Ö –∏ –ø–æ —Å—Ä–µ–¥–Ω–µ–º—É |—Å–∏–≥–Ω–∞–ª–∞|
                    tmp = test_data[['ticker']].copy()
                    tmp['news_signal'] = news_signal.values
                    by_ticker = tmp.groupby('ticker')
                    top_cov = by_ticker['news_signal'].apply(lambda s: float((s != 0).mean())).sort_values(ascending=False).head(10)
                    top_mag = by_ticker['news_signal'].apply(lambda s: float(s.abs().mean())).sort_values(ascending=False).head(10)
                    print("   üîù –¢–∏–∫–µ—Ä—ã –ø–æ coverage:")
                    for t, v in top_cov.items():
                        print(f"      {t}: {v:.3f}")
                    print("   üîù –¢–∏–∫–µ—Ä—ã –ø–æ —Å—Ä–µ–¥–Ω–µ–º—É |news_signal|:")
                    for t, v in top_mag.items():
                        print(f"      {t}: {v:.4f}")
            except Exception as _diag_err:
                print(f"   ‚ö†Ô∏è –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ news_signal –ø—Ä–æ–ø—É—â–µ–Ω–∞: {_diag_err}")

            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏ —Å –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–µ–π –æ—Ç –Ω–æ–≤–æ—Å—Ç–µ–π (–∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–æ)
            # –î–ª—è 1 –¥–Ω—è: momentum * (0.3 + 0.1 * clip(news_signal))
            # –î–ª—è 20 –¥–Ω–µ–π: momentum * (1.0 + 0.2 * clip(news_signal))
            if isinstance(news_signal, (int, float)):
                ns_1d = 0.0
                ns_20d = 0.0
            else:
                ns_1d = news_signal.clip(-1.0, 1.0) * 0.1
                ns_20d = news_signal.clip(-1.0, 1.0) * 0.2

            test_data['pred_return_1d'] = test_data['momentum'] * (0.3 + (ns_1d if not isinstance(ns_1d, float) else 0.0))
            test_data['pred_return_20d'] = test_data['momentum'] * (1.0 + (ns_20d if not isinstance(ns_20d, float) else 0.0))

            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Ä–æ—Å—Ç–∞
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–∏–≥–º–æ–∏–¥—É –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –º–æ–º–µ–Ω—Ç—É–º–∞ –≤ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å
            def sigmoid(x, sensitivity=10):
                return 1 / (1 + np.exp(-sensitivity * x))

            # –í–ª–∏—è–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–Ω–æ–≥–æ —Å–∏–≥–Ω–∞–ª–∞ –Ω–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Ä–æ—Å—Ç–∞
            if isinstance(news_signal, (int, float)):
                test_data['pred_prob_up_1d'] = sigmoid(test_data['momentum'], sensitivity=10)
                test_data['pred_prob_up_20d'] = sigmoid(test_data['momentum'], sensitivity=5)
            else:
                test_data['pred_prob_up_1d'] = sigmoid(test_data['momentum'] + 0.2 * news_signal, sensitivity=10)
                test_data['pred_prob_up_20d'] = sigmoid(test_data['momentum'] + 0.1 * news_signal, sensitivity=5)

        # Clipping: –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ [0.1, 0.9] –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        test_data['pred_prob_up_1d'] = test_data['pred_prob_up_1d'].clip(0.1, 0.9)
        test_data['pred_prob_up_20d'] = test_data['pred_prob_up_20d'].clip(0.1, 0.9)

        # Clipping: –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏ –≤ —Ä–∞–∑—É–º–Ω–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ [-0.2, 0.2]
        test_data['pred_return_1d'] = test_data['pred_return_1d'].clip(-0.2, 0.2)
        test_data['pred_return_20d'] = test_data['pred_return_20d'].clip(-0.5, 0.5)

        self.predictions = test_data

        print(f"   ‚úì –°–æ–∑–¥–∞–Ω–æ {len(self.predictions)} –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π")
        print(f"\n    –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π:")
        print(f"      –°—Ä–µ–¥–Ω—è—è pred_return_1d:  {test_data['pred_return_1d'].mean():.6f}")
        print(f"      –°—Ä–µ–¥–Ω—è—è pred_return_20d: {test_data['pred_return_20d'].mean():.6f}")
        print(f"      –°—Ä–µ–¥–Ω—è—è pred_prob_up_1d: {test_data['pred_prob_up_1d'].mean():.4f}")
        print(f"      –°—Ä–µ–¥–Ω—è—è pred_prob_up_20d: {test_data['pred_prob_up_20d'].mean():.4f}")

    def save_submission(self, output_path: str = "submission.csv"):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ submission —Ñ–∞–π–ª–∞"""
        print(f"\n –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ submission...")

        submission = self.predictions[[
            'ticker', 'begin',
            'pred_return_1d', 'pred_return_20d',
            'pred_prob_up_1d', 'pred_prob_up_20d'
        ]].copy()

        submission.to_csv(output_path, index=False)

        print(f"   ‚úì Submission —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_path}")
        print(f"   –°—Ç—Ä–æ–∫: {len(submission)}")
        print(f"\n   üìã –ü–µ—Ä–≤—ã–µ —Å—Ç—Ä–æ–∫–∏:")
        print(submission.head(10).to_string(index=False))

    def run(self, train_candles_path: str, test_candles_path: str,
            train_news_path: str = None, test_news_path: str = None,
            output_path: str = "submission.csv", use_meta_stock: bool = True):
        """–ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω baseline —Ä–µ—à–µ–Ω–∏—è —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π META-Stock"""
        print("=" * 70)
        print("üöÄ BASELINE –†–ï–®–ï–ù–ò–ï + META-STOCK")
        print("=" * 70 + "\n")

        # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        self.load_data(train_candles_path, test_candles_path, train_news_path, test_news_path)

        if use_meta_stock:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º META-Stock –ø–æ–¥—Ö–æ–¥ –¥–ª—è –º—É–ª—å—Ç–∏-–≥–æ—Ä–∏–∑–æ–Ω—Ç–Ω–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è
            print("\nü§ñ –ò—Å–ø–æ–ª—å–∑—É–µ–º META-Stock –ø–æ–¥—Ö–æ–¥...")
            meta_predictor = METAStockPredictor(horizons=list(range(1, 21)), n_splits=5)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ø—Ä–∏–±—ã–ª–∏ –¥–ª—è –≤—Å–µ—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤
            predictions_df = meta_predictor.predict_returns(self.full_df)
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è submission
            test_mask = self.full_df['begin'].isin(self.test_df['begin'])
            test_predictions = predictions_df[test_mask].copy()
            
            # –°–æ–∑–¥–∞–µ–º submission –≤ –Ω—É–∂–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
            submission_cols = ['ticker', 'begin']
            for horizon in [1, 20]:  # –û—Å–Ω–æ–≤–Ω—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã –¥–ª—è submission
                submission_cols.extend([
                    f'pred_return_{horizon}d',
                    f'pred_prob_up_{horizon}d'
                ])
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Ä–æ—Å—Ç–∞ (–Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–π –ø—Ä–∏–±—ã–ª–∏)
            for horizon in [1, 20]:
                # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Ä–æ—Å—Ç–∞ = —Å–∏–≥–º–æ–∏–¥–∞ –æ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–π –ø—Ä–∏–±—ã–ª–∏
                returns = test_predictions[f'pred_return_{horizon}d']
                test_predictions[f'pred_prob_up_{horizon}d'] = 1 / (1 + np.exp(-10 * returns))
            
            submission = test_predictions[submission_cols].copy()
            
        else:
            # –°—Ç–∞—Ä—ã–π –ø–æ–¥—Ö–æ–¥
            # 2. –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            self.compute_features()

            # 3. –û–±—É—á–µ–Ω–∏–µ ML –º–æ–¥–µ–ª–µ–π (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ)
            if self.use_ml:
                self.train_ml_models()

            # 4. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            self.predict()

            # 5. –°–æ–∑–¥–∞–Ω–∏–µ submission
            submission = self.predictions[[
                'ticker', 'begin',
                'pred_return_1d', 'pred_return_20d',
                'pred_prob_up_1d', 'pred_prob_up_20d'
            ]].copy()

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        submission.to_csv(output_path, index=False)
        
        print(f"\nüíæ Submission —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_path}")
        print(f"   –°—Ç—Ä–æ–∫: {len(submission)}")
        print(f"\n   üìã –ü–µ—Ä–≤—ã–µ —Å—Ç—Ä–æ–∫–∏:")
        print(submission.head(10).to_string(index=False))

        print("\n" + "=" * 70)
        print("‚úÖ BASELINE + META-STOCK –ì–û–¢–û–í!")
        print("=" * 70)
        print(f"\nüí° –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ —Ä–µ—à–µ–Ω–∏—è:")
        print(f"   ‚Ä¢ META-Stock –ø–æ–¥—Ö–æ–¥ –¥–ª—è –º—É–ª—å—Ç–∏-–≥–æ—Ä–∏–∑–æ–Ω—Ç–Ω–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è")
        print(f"   ‚Ä¢ TimeSeriesSplit –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–∏")
        print(f"   ‚Ä¢ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ø—Ä–∏–±—ã–ª–∏ –Ω–∞ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞—Ö 1-20 –¥–Ω–µ–π")
        print(f"   ‚Ä¢ –ö–æ–º–±–∏–Ω–∞—Ü–∏—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ –∏ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö —Ñ–∏—á")
        print(f"   ‚Ä¢ MultiOutputRegressor –¥–ª—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤—Å–µ—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤")


class AdvancedSolution:
    """
    –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ —Ä–µ—à–µ–Ω–∏–µ, –æ–±—ä–µ–¥–∏–Ω—è—é—â–µ–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã, –Ω–æ–≤–æ—Å—Ç–Ω—ã–µ —Ñ–∏—á–∏ –∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –±—É—Å—Ç–∏–Ω–≥.
    
    –ö–ª—é—á–µ–≤—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
    - –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã: SMA, EMA, ROC
    - –ù–æ–≤–æ—Å—Ç–Ω—ã–µ —Ñ–∏—á–∏: TF-IDF + —Å–µ–Ω—Ç–∏–º–µ–Ω—Ç (—Å fallback –Ω–∞ news_features_no_ticker)
    - –í—Ä–µ–º–µ–Ω–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è —Å TimeSeriesSplit
    - –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –±—É—Å—Ç–∏–Ω–≥ —Å –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π
    - –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –¥–∞–Ω–Ω—ã—Ö
    """
    
    def __init__(self, test_size: float = 0.2, n_splits: int = 3):
        """
        Args:
            test_size: –î–æ–ª—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è hold-out —Ç–µ—Å—Ç–∞
            n_splits: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ø–ª–∏—Ç–æ–≤ –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        """
        self.test_size = test_size
        self.n_splits = n_splits
        self.model = None
        self.scaler = None
        self.feature_names = []
        
    def compute_technical_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞"""
        df = df.sort_values('begin').copy()
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ç–∏–∫–µ—Ä—É
        group_cols = ['ticker'] if 'ticker' in df.columns else None
        
        def _indicators(group: pd.DataFrame) -> pd.DataFrame:
            group = group.copy()
            group['sma_5'] = group['close'].rolling(window=5).mean()
            group['sma_10'] = group['close'].rolling(window=10).mean()
            group['ema_5'] = group['close'].ewm(span=5, adjust=False).mean()
            group['ema_10'] = group['close'].ewm(span=10, adjust=False).mean()
            group['roc_1'] = group['close'].pct_change(periods=1)
            group['roc_5'] = group['close'].pct_change(periods=5)
            return group
        
        if group_cols is not None:
            df = df.groupby(group_cols).apply(_indicators).reset_index(drop=True)
        else:
            df = _indicators(df)
        
        # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN –∑–Ω–∞—á–µ–Ω–∏—è
        df = df.fillna(method='bfill').fillna(method='ffill')
        return df
    
    def extract_news_features(self, news_df: pd.DataFrame) -> pd.DataFrame:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö —Ñ–∏—á —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ daily_features"""
        news_df = news_df.copy()
        news_df['publish_date'] = pd.to_datetime(news_df['publish_date'])
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é daily_features
        try:
            feat_df = daily_features(news_df, lookback_days=20, taus=(1.0, 5.0))
            if 'day' in feat_df.columns:
                feat_df['dt'] = pd.to_datetime(feat_df['day'])
            return feat_df
        except Exception as e:
            print(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö —Ñ–∏—á: {e}")
            # Fallback: –±–∞–∑–æ–≤—ã–µ TF-IDF —Ñ–∏—á–∏
            return self._extract_basic_news_features(news_df)
    
    def _extract_basic_news_features(self, news_df: pd.DataFrame) -> pd.DataFrame:
        """Fallback: –±–∞–∑–æ–≤—ã–µ TF-IDF —Ñ–∏—á–∏"""
        text_data = news_df['title'].fillna('')
        if 'publication' in news_df.columns:
            text_data = text_data + ' ' + news_df['publication'].fillna('')
        
        # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
        vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
        X_text = vectorizer.fit_transform(text_data)
        
        # –ü—Ä–æ—Å—Ç–æ–π —Å–µ–Ω—Ç–∏–º–µ–Ω—Ç —Å–∫–æ—Ä
        positive_words = set(['—Ä–æ—Å—Ç', '–ø–æ–≤—ã—à–µ–Ω–∏–µ', '–ø—Ä–∏–±—ã–ª—å', '—É—Å–ø–µ—Ö', '–ø—Ä–æ—Ä—ã–≤', 'up', 'rise', 'gain'])
        negative_words = set(['–ø–∞–¥–µ–Ω–∏–µ', '—É–±—ã—Ç–æ–∫', '–∫—Ä–∏–∑–∏—Å', '—Å–Ω–∏–∂–µ–Ω–∏–µ', '—Ä–µ—Ü–µ—Å—Å–∏—è', 'down', 'fall', 'loss'])
        
        def sentiment_score(text: str) -> int:
            tokens = text.lower().split()
            pos_count = sum(1 for tok in tokens if tok in positive_words)
            neg_count = sum(1 for tok in tokens if tok in negative_words)
            return pos_count - neg_count
        
        news_df['sentiment'] = text_data.apply(sentiment_score)
        
        # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –ø–æ –¥–Ω—è–º
        news_df['date_only'] = news_df['publish_date'].dt.date
        aggregated_features = []
        feature_names = [f'tfidf_{i}' for i in range(X_text.shape[1])]
        
        for date, idx in news_df.groupby('date_only').groups.items():
            tfidf_vec = X_text[idx].mean(axis=0)
            sentiment = news_df.loc[idx, 'sentiment'].sum()
            
            row = {'dt': pd.Timestamp(date), 'sentiment_sum': sentiment}
            row.update({name: tfidf_vec[0, j] for j, name in enumerate(feature_names)})
            aggregated_features.append(row)
        
        feat_df = pd.DataFrame(aggregated_features)
        return feat_df
    
    def build_dataset(self, candles_path: str, news_path: str) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å–≤–µ—á–µ–π –∏ –Ω–æ–≤–æ—Å—Ç–µ–π"""
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        candles = pd.read_csv(candles_path)
        news = pd.read_csv(news_path)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–ª–æ–Ω–æ–∫ –¥–∞—Ç
        for col in ['begin', 'dt', 'date', 'timestamp']:
            if col in candles.columns:
                candles['dt'] = pd.to_datetime(candles[col])
                break
        
        for col in ['publish_date', 'dt', 'date', 'timestamp']:
            if col in news.columns:
                news['publish_date'] = pd.to_datetime(news[col])
                break
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
        candles = self.compute_technical_indicators(candles)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞—Ä–≥–µ—Ç–∞: —Ü–µ–Ω–∞ –≤—ã—Ä–∞—Å—Ç–µ—Ç –≤ —Å–ª–µ–¥—É—é—â–∏–π –ø–µ—Ä–∏–æ–¥?
        candles = candles.sort_values('dt')
        candles['target_up'] = (candles['close'].shift(-1) > candles['close']).astype(int)
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ –±–µ–∑ —Ç–∞—Ä–≥–µ—Ç–∞
        candles = candles.dropna(subset=['target_up'])
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö —Ñ–∏—á
        news_feats = self.extract_news_features(news)
        
        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–æ –¥–∞—Ç–µ
        candles['date_only'] = candles['dt'].dt.date
        news_feats['date_only'] = news_feats['dt'].dt.date
        
        merged = candles.merge(news_feats.drop(columns='dt'), on='date_only', how='left')
        merged = merged.sort_values('dt')
        
        # Forward fill –¥–ª—è –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö —Ñ–∏—á
        news_feature_cols = [c for c in merged.columns if c.startswith('tfidf_') or c.startswith('sentiment') or c.startswith('news_')]
        merged[news_feature_cols] = merged[news_feature_cols].fillna(method='ffill').fillna(0)
        
        # –£–¥–∞–ª—è–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        merged = merged.drop(columns=['date_only'])
        return merged
    
    def train_model(self, dataset: pd.DataFrame) -> tuple:
        """–û–±—É—á–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ –±—É—Å—Ç–∏–Ω–≥–∞ —Å –≤—Ä–µ–º–µ–Ω–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π"""
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ —Ç–∞—Ä–≥–µ—Ç
        X = dataset.drop(columns=['target_up', 'dt', 'begin'])
        y = dataset['target_up']
        
        # –í—ã–±–æ—Ä —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        num_cols = X.select_dtypes(include=[np.number]).columns
        X = X[num_cols]
        self.feature_names = list(X.columns)
        
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        self.scaler = StandardScaler()
        X_scaled = self.scaler.fit_transform(X)
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/hold-out
        split_idx = int(len(X_scaled) * (1 - self.test_size))
        X_train, X_test = X_scaled[:split_idx], X_scaled[split_idx:]
        y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]
        
        # –í—Ä–µ–º–µ–Ω–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –¥–ª—è –≤—ã–±–æ—Ä–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        tscv = TimeSeriesSplit(n_splits=self.n_splits)
        best_model = None
        best_score = -np.inf
        
        # –°–µ—Ç–∫–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        learning_rates = [0.05, 0.1]
        n_estimators_list = [50, 100]
        max_depths = [3, 5]
        
        print(f"    –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–∞ {len(X_train)} –ø—Ä–∏–º–µ—Ä–∞—Ö...")
        
        for lr in learning_rates:
            for n_estimators in n_estimators_list:
                for depth in max_depths:
                    model = GradientBoostingClassifier(
                        learning_rate=lr,
                        n_estimators=n_estimators,
                        max_depth=depth,
                        random_state=42,
                    )
                    
                    cv_scores = []
                    for train_idx, val_idx in tscv.split(X_train):
                        X_tr, X_val = X_train[train_idx], X_train[val_idx]
                        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]
                        
                        model.fit(X_tr, y_tr)
                        y_pred_proba = model.predict_proba(X_val)[:, 1]
                        y_pred = (y_pred_proba > 0.5).astype(int)
                        acc = accuracy_score(y_val, y_pred)
                        cv_scores.append(acc)
                    
                    mean_score = np.mean(cv_scores)
                    if mean_score > best_score:
                        best_score = mean_score
                        best_model = model
        
        # –û–±—É—á–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ –Ω–∞ –≤—Å–µ–º train –Ω–∞–±–æ—Ä–µ
        assert best_model is not None
        best_model.fit(X_train, y_train)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ hold-out –Ω–∞–±–æ—Ä–µ
        y_pred_test = best_model.predict_proba(X_test)[:, 1]
        
        print(f"   ‚úì –õ—É—á—à–∏–π CV score: {best_score:.4f}")
        print(f"   ‚úì Hold-out accuracy: {accuracy_score(y_test, (y_pred_test > 0.5).astype(int)):.4f}")
        
        self.model = best_model
        return best_model, y_test, pd.Series(y_pred_test, index=y_test.index)
    
    def run(self, train_candles_path: str, test_candles_path: str,
            train_news_path: str = None, test_news_path: str = None,
            output_path: str = "advanced_submission.csv"):
        """–ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è —Å –Ω–æ–≤–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π –¥–∞–Ω–Ω—ã—Ö"""
        print("=" * 70)
        print("üöÄ ADVANCED SOLUTION (Gradient Boosting + News)")
        print("=" * 70 + "\n")
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º train –∏ test –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        print("üìä –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
        train_dataset = self.build_dataset(train_candles_path, train_news_path)
        test_dataset = self.build_dataset(test_candles_path, test_news_path)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ)
        full_dataset = pd.concat([train_dataset, test_dataset], ignore_index=True)
        
        print(f"   ‚úì Train: {len(train_dataset)} —Å—Ç—Ä–æ–∫")
        print(f"   ‚úì Test: {len(test_dataset)} —Å—Ç—Ä–æ–∫")
        print(f"   ‚úì Features: {len([c for c in full_dataset.columns if c not in ['target_up', 'dt', 'begin']])}")
        
        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        print("\nü§ñ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
        model, y_true, y_pred = self.train_model(full_dataset)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ submission —Ñ–∞–π–ª–∞ –≤ –Ω—É–∂–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
        print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ submission...")
        
        # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ test –¥–∞–Ω–Ω—ã–µ –¥–ª—è submission
        test_mask = full_dataset['dt'].isin(test_dataset['dt'])
        test_subset = full_dataset[test_mask].copy()
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è test –¥–∞–Ω–Ω—ã—Ö
        X_test = test_subset[self.feature_names]
        X_test_scaled = self.scaler.transform(X_test)
        
        test_subset['pred_return_1d'] = model.predict_proba(X_test_scaled)[:, 1] * 0.1  # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º
        test_subset['pred_return_20d'] = model.predict_proba(X_test_scaled)[:, 1] * 0.2
        test_subset['pred_prob_up_1d'] = model.predict_proba(X_test_scaled)[:, 1]
        test_subset['pred_prob_up_20d'] = model.predict_proba(X_test_scaled)[:, 1]
        
        # –ö–ª–∏–ø–ø–∏–Ω–≥
        test_subset['pred_return_1d'] = test_subset['pred_return_1d'].clip(-0.2, 0.2)
        test_subset['pred_return_20d'] = test_subset['pred_return_20d'].clip(-0.5, 0.5)
        test_subset['pred_prob_up_1d'] = test_subset['pred_prob_up_1d'].clip(0.1, 0.9)
        test_subset['pred_prob_up_20d'] = test_subset['pred_prob_up_20d'].clip(0.1, 0.9)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –Ω—É–∂–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
        submission = test_subset[[
            'ticker', 'begin',
            'pred_return_1d', 'pred_return_20d',
            'pred_prob_up_1d', 'pred_prob_up_20d'
        ]].copy()
        
        submission.to_csv(output_path, index=False)
        
        print(f"   ‚úì Submission —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_path}")
        print(f"   –°—Ç—Ä–æ–∫: {len(submission)}")
        
        print("\n" + "=" * 70)
        print("‚úÖ ADVANCED SOLUTION –ì–û–¢–û–í!")
        print("=" * 70)


def run_baseline_solution():
    """–ó–∞–ø—É—Å–∫ BaselineSolution —Å META-Stock –ø–æ–¥—Ö–æ–¥–æ–º"""
    baseline = BaselineSolution(window_size=5, use_ml=True)
    baseline.run(
        train_candles_path="/Users/safroelena/Desktop/—Ñ–∏–Ω-—Ö–∞–∫/finam-x-hse-trade-ai-hack-forecast/data/raw/participants/candles.csv",
        test_candles_path="/Users/safroelena/Desktop/—Ñ–∏–Ω-—Ö–∞–∫/finam-x-hse-trade-ai-hack-forecast/data/raw/participants/candles_2.csv",
        train_news_path="/Users/safroelena/Desktop/—Ñ–∏–Ω-—Ö–∞–∫/finam-x-hse-trade-ai-hack-forecast/data/raw/participants/news.csv",
        test_news_path="/Users/safroelena/Desktop/—Ñ–∏–Ω-—Ö–∞–∫/finam-x-hse-trade-ai-hack-forecast/data/raw/participants/news_2.csv",
        output_path="meta_stock_submission.csv",
        use_meta_stock=True
    )


def run_advanced_solution():
    """–ó–∞–ø—É—Å–∫ AdvancedSolution —Å –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–º –±—É—Å—Ç–∏–Ω–≥–æ–º"""
    solution = AdvancedSolution(test_size=0.2, n_splits=3)
    solution.run(
        train_candles_path="/Users/safroelena/Desktop/—Ñ–∏–Ω-—Ö–∞–∫/finam-x-hse-trade-ai-hack-forecast/data/raw/participants/candles.csv",
        test_candles_path="/Users/safroelena/Desktop/—Ñ–∏–Ω-—Ö–∞–∫/finam-x-hse-trade-ai-hack-forecast/data/raw/participants/candles_2.csv",
        train_news_path="/Users/safroelena/Desktop/—Ñ–∏–Ω-—Ö–∞–∫/finam-x-hse-trade-ai-hack-forecast/data/raw/participants/news.csv",
        test_news_path="/Users/safroelena/Desktop/—Ñ–∏–Ω-—Ö–∞–∫/finam-x-hse-trade-ai-hack-forecast/data/raw/participants/news_2.csv",
        output_path="advanced_submission.csv"
    )


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "advanced":
        print("üöÄ –ó–∞–ø—É—Å–∫ AdvancedSolution...")
        run_advanced_solution()
    else:
        print("üöÄ –ó–∞–ø—É—Å–∫ BaselineSolution...")
        run_baseline_solution()

